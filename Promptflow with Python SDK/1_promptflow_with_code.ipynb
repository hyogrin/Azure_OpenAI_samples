{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Run chat flows using Promptflow Python SDK \n",
    "프롬프트 플로우는 아이디어 발상, 프로토타이핑, 테스트, 평가부터 프로덕션 배포 및 모니터링에 이르기까지 LLM 기반 AI 애플리케이션의 엔드투엔드 개발 주기를 간소화하도록 설계된 개발 도구 모음입니다. 이를 통해 신속한 엔지니어링이 훨씬 쉬워지고 프로덕션 수준의 품질로 LLM 앱을 빌드할 수 있습니다. \n",
    "\n",
    "> ✨ ***Note*** <br>\n",
    "> Promptflow는 오픈소스 프로젝트이며, Azure ML, Azure AI Studio, Local VS Code환경을 모두 지원합니다.  \n",
    "\n",
    "## Prerequisites\n",
    "3.10 또는 이후 버전의 Python 가상 환경을 구성합니다 : \n",
    " 1. 명령 Palette를 엽니다 (Ctrl+Shift+P).\n",
    " 1. Python을 검색합니다: Create Environment를 선택합니다.\n",
    " 1. Venv / Conda를 선택하고 새 환경을 만들 위치를 선택합니다.\n",
    " 1. Python interpreter 버전을 선택합니다. 3.10 이상의 버전으로 생성합니다.\n",
    "\n",
    "디펜던시 설치를 위해 아래 코드를 실행하여 실행에 필요한 package들을 설치하세요. \n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "## Set up your environment\n",
    "현재의 repository를 Clone 하세요. \n",
    "\n",
    "```bash\n",
    "git clone https://github.com/hyogrin/Azure_OpenAI_samples.git\n",
    "```\n",
    "\n",
    ".env-sample 파일을 기반으로 .env 파일을 만듭니다. 새 .env 파일을 노트북이 들어 있는 폴더에 복사하고 변수를 업데이트합니다.\n",
    "\n",
    "## Steps\n",
    "#### 1. Set up Promptflow client with Credential and configuration\n",
    "#### 2. Create a new chat flow by providing the flow name and description.\n",
    "#### 3. Run Basic Promptflow with questions to compare models\n",
    "#### 4. Run Context Added Promptflow with the outdoor questions \n",
    "#### 5. Use serverless endpoint to run the Promptflow with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Import required libraries\n",
    "from promptflow.azure import PFClient\n",
    "from promptflow.entities import Run\n",
    "# Import required libraries\n",
    "from azure.identity import DefaultAzureCredential, EnvironmentCredential, InteractiveBrowserCredential\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "load_dotenv()\n",
    "with open('./config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "print(config[\"subscription_id\"])\n",
    "print(config[\"resource_group\"])\n",
    "print(config[\"workspace_name\"]) # Azure AI Studio project name which is not the same as the Azure ML workspace name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Monitor the status of the run_result\n",
    "def monitor_status(pf_azure_client:PFClient, run_result:Run):\n",
    "    with tqdm(total=3, desc=\"Running Status\", unit=\"step\") as pbar:\n",
    "        status = pf_azure_client.runs.get(run_result).status\n",
    "        if status == \"Preparing\":\n",
    "            pbar.update(1)\n",
    "        while status != \"Completed\" and status != \"Failed\":\n",
    "            if status == \"Running\" and pbar.n < 2:\n",
    "                pbar.update(1)\n",
    "            print(f\"Current Status: {status}\")\n",
    "            time.sleep(10)\n",
    "            status = pf_azure_client.runs.get(run_result).status\n",
    "        pbar.update(1)\n",
    "        print(\"Promptflow Running Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up Promptflow client with Credential and configuration\n",
    "- Create a promptflow client with the credential and configuration. You need to set the `config.json` file with subscription_id, resource_group and workspace_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "# if you cannot use DefaultAzureCredential and InteractiveBrowserCredential you need to set up the Managed identity in your .env file\n",
    "\n",
    "pf_azure_client = PFClient.from_config(credential=credential, path=\"./config.json\")\n",
    "\n",
    "# pf_azure_client = PFClient(credential=credential, \n",
    "#                            subscription_id=\"your subscription id\", \n",
    "#                            resource_group_name=\"your resource group name\", \n",
    "#                            workspace_name=\"your workspace name\")            \n",
    "\n",
    "try:\n",
    "    workspace = pf_azure_client.ml_client.workspaces.get(name=config[\"workspace_name\"])\n",
    "    print(f\"Connected to Azure AI Studio Workspace: {workspace.name}\")\n",
    "    print(f\"Workspace Location: {workspace.location}\")\n",
    "    print(f\"Workspace ID: {workspace.id}\")\n",
    "except HttpResponseError as e:\n",
    "    print(f\"Failed to connect to Azure ML Workspace: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a new chat flow by providing the flow name and description.\n",
    "\n",
    "-   Create a new chat flow by providing the flow name and description. You can view and clone the flow on Azure AI studio UI.\n",
    "\n",
    "> ✨ **_important_** <br>\n",
    "> Grant the Storage File Data Privileged Contributor, Storage Blob Data Contributor at the storage of AI studiorole to user, group, service principle and managed Identity which you are trying to access, you can execute the code below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the exist connections\n",
    "- currently we only support create connection in Azure AI, ML Studio UI. Check the exiting connections in the workspace.\n",
    "> ✨ ***important*** <br>\n",
    "> Update flow.dag.yaml files in your flow_path with the connection name you have created in the Azure ML Studio UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Environment, FileSystemLoader\n",
    "from pathlib import Path\n",
    "\n",
    "env = Environment(loader=FileSystemLoader('.'))\n",
    "\n",
    "# Read the template file\n",
    "template = env.get_template('./flow-template/chat.flow.dag.yaml')\n",
    "\n",
    "# Define the variables for the template\n",
    "variables = {\n",
    "\t\"your_aoai_connection_name\": \"replace with your connection name\",\n",
    "}\n",
    "\n",
    "rendered_content = template.render(variables)\n",
    "Path('./chat/flow.dag.yaml').write_text(rendered_content)\n",
    "\n",
    "print(Path('./chat/flow.dag.yaml').read_text()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grant the right accessibility to create the flow \n",
    "pf_azure_client.flows.create_or_update(flow=\"chat/\", type=\"chat\", display_name=\"comparison flow created from python sdk\", description=\"fine-tuned model comparison flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Basic Promptflow with questions to compare models\n",
    "\n",
    "-   Run the Promptflow with the simple questions such as \"What is the capital of France?\" and compare the results of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_path = \"./chat\"\n",
    "data_path = \"./data/questions_basic.jsonl\"\n",
    "\n",
    "column_mapping = {\n",
    "    \"question\": \"${data.question}\"\n",
    "}\n",
    "\n",
    "run_result = pf_azure_client.run(\n",
    "    flow=flow_path,\n",
    "    type=\"chat\",\n",
    "    data=data_path,\n",
    "    column_mapping=column_mapping,\n",
    "    display_name=\"chat_with_data\",\n",
    "    tags={\"chat_with_jsonl\": \"\", \"1st_round\": \"\"},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_status(pf_azure_client, run_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detail = pf_azure_client.get_details(run_result)\n",
    "\n",
    "detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Context Added Promptflow with the outdoor questions\n",
    "\n",
    "-   Run the Promptflow using the context data and ask the outdoor product related questions to compare the results of the models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the exist connections\n",
    "\n",
    "-   currently we only support create connection in Azure AI, ML Studio UI. Check the exiting connections in the workspace.\n",
    "    > ✨ **_important_** <br>\n",
    "    > Update flow.dag.yaml files in your flow_path with the connection name you have created in the Azure ML Studio UI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Environment, FileSystemLoader\n",
    "from pathlib import Path\n",
    "\n",
    "env = Environment(loader=FileSystemLoader('.'))\n",
    "\n",
    "# Read the template file\n",
    "template = env.get_template('./flow-template/chat-context.flow.dag.yaml')\n",
    "\n",
    "# Define the variables for the template\n",
    "variables = {\n",
    "\t\"your_aoai_connection_name\": \"replace with your connection name\",\n",
    "}\n",
    "\n",
    "rendered_content = template.render(variables)\n",
    "Path('./chat-context/flow.dag.yaml').write_text(rendered_content)\n",
    "\n",
    "print(Path('./chat-context/flow.dag.yaml').read_text()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_path = \"./chat-context\"\n",
    "data_path = \"./data/questions_outdoor.jsonl\"\n",
    "\n",
    "# get the context from context.json file as str and map it to the column_mapping\n",
    "with open('./data/context_simple.json', 'r') as file:\n",
    "    context = json.load(file)\n",
    "\n",
    "column_mapping = {\n",
    "    \"question\": \"${data.question}\",\n",
    "    \"context\": context.get(\"context\")    \n",
    "}\n",
    "\n",
    "run_result_with_context = pf_azure_client.run(\n",
    "    flow=flow_path,\n",
    "    type=\"chat\",\n",
    "    data=data_path, \n",
    "    column_mapping=column_mapping,\n",
    "    display_name=\"chat_context_data\",\n",
    "    tags={\"chat_with_context_jsonl\": \"\", \"1st_round\": \"\"},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_status(pf_azure_client, run_result_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detail = pf_azure_client.get_details(run_result_with_context)\n",
    "\n",
    "detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use serverless endpoint to run the Promptflow with context\n",
    "- Create a serverless endpoint to run the Promptflow with the context. You can use the endpoint to run the flow with the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deploy your serverless endpoint\n",
    "\n",
    "- go to the Azure AI studio > Model catalog > search phi-3.5 > deply Phi-3.5-mini-instruct as your serverless endpint \n",
    "<br>\n",
    "![serverless endpoint](./images/deploy_serverless_endpoint.jpg)\n",
    "<br>\n",
    "<br>\n",
    "- once the deployment is done, go to Deployments and you can see the endpoint deployed in the endpoint section. Click to check the details and copy key and phi35-mini-instruct: Chat Completion endpoint url\n",
    "![copy connection](./images/copy_connection.jpg)\n",
    "<br>\n",
    "<br>\n",
    "- go to Settings in Azure AI studio > Connections > create a new connection naming phi35-serverless with the copied key and endpoint url\n",
    "![create new serverless connection](./images/create_new_serverless_connection.jpg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the exist connections\n",
    "- currently we only support create connection in Azure AI, ML Studio UI. Check the exiting connections in the workspace.\n",
    "> ✨ ***important*** <br>\n",
    "> Update flow.dag.yaml files in your flow_path with the connection name you have created in the Azure ML Studio UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Environment, FileSystemLoader\n",
    "from pathlib import Path\n",
    "\n",
    "env = Environment(loader=FileSystemLoader('.'))\n",
    "\n",
    "# Read the template file\n",
    "template = env.get_template('./flow-template/chat-serverless.flow.dag.yaml')\n",
    "\n",
    "# Define the variables for the template with your connection names for chat serverless \n",
    "variables = {\n",
    "\t\"your_phi35_serverless_connection_name\": \"replace with your connection name\",\n",
    "\t\"your_aoai_connection_name\": \"replace with your connection name\",\n",
    "}\n",
    "\n",
    "rendered_content = template.render(variables)\n",
    "Path('./chat-serverless/flow.dag.yaml').write_text(rendered_content)\n",
    "\n",
    "print(Path('./chat-serverless/flow.dag.yaml').read_text()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_path = \"./chat-serverless\"\n",
    "data_path = \"./data/questions_outdoor.jsonl\"\n",
    "\n",
    "# get the context from context.json file as str and map it to the column_mapping\n",
    "with open('./data/context_simple.json', 'r') as file:\n",
    "    context = json.load(file)\n",
    "\n",
    "column_mapping = {\n",
    "    \"question\": \"${data.question}\",\n",
    "    \"context\": context.get(\"context\")    \n",
    "}\n",
    "\n",
    "run_serverless_result = pf_azure_client.run(\n",
    "    flow=flow_path,\n",
    "    type=\"chat\",\n",
    "    data=data_path, \n",
    "    column_mapping=column_mapping,\n",
    "    display_name=\"chat_serverless_context_data\",\n",
    "    tags={\"chat_serverless_context_jsonl\": \"\", \"1st_round\": \"\"},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_status(pf_azure_client, run_serverless_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detail = pf_azure_client.get_details(run_serverless_result)\n",
    "\n",
    "detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
