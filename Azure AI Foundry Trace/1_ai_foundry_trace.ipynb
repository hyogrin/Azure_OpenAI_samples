{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Foundry trace \n",
    "You can trace your application with Azure AI Foundry project library. This provides support for tracing with OpenTelemetry.\n",
    "\n",
    "> ‚ú® ***Note*** <br>\n",
    "> Please check the prerequisites and set up your environment before running the code below. First follow steps to create an AI Project if you don't have one already. To enable tracing, first ensure your project has an attached Application Insights resource. Go to the Tracing page of your project in Azure AI Foundry portal and follow instructions to create or attach Application Insights.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "Configure a Python virtual environment for 3.10 or later: \n",
    " 1. An Azure Subscription.\n",
    " 1. An Azure AI project, see Create a project in Azure AI Foundry portal.\n",
    " 1. An AI model supporting the Azure AI model inference API deployed through Azure AI Foundry.\n",
    " 1. Ensure your project has an attached Application Insights resource\n",
    " 1. Open the Command Palette (Ctrl+Shift+P).\n",
    " 1. Search for Python: Create Environment.\n",
    " 1. select Venv / Conda and choose where to create the new environment.\n",
    " 1. Select the Python interpreter version. Create with version 3.10 or later.\n",
    "\n",
    "For a dependency installation, run the code below to install the packages required to run it. \n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "## Set up your environment\n",
    "Git clone the repository to your local machine. \n",
    "\n",
    "```bash\n",
    "git clone https://github.com/hyogrin/Azure_OpenAI_samples.git\n",
    "```\n",
    "\n",
    "Create an .env file based on the .env-sample file. Copy the new .env file to the folder containing your notebook and update the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from opentelemetry import trace\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.inference.models import (\n",
    "    ToolMessage,\n",
    "    AssistantMessage,\n",
    "    ChatCompletionsToolCall,\n",
    "    ChatCompletionsToolDefinition,\n",
    "    FunctionDefinition,\n",
    ")\n",
    "\n",
    "# Install opentelemetry with command \"pip install opentelemetry-sdk\".\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter\n",
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage, CompletionsFinishReason\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "azure_model_inference_endpoint = os.getenv(\"AZURE_MODEL_INFERENCE_ENDPOINT\")\n",
    "aoai_api_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "aoai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "aoai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "aoai_deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "project_connection_string = os.getenv(\"PROJECT_CONNECTION_STRING\")\n",
    "application_insights_connection_string = os.getenv(\"APPLICATIONINSIGHTS_CONNECTION_STRING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Copy the connection string from your Application Insights resource\n",
    "The connection string is unique and specifies where the Azure Monitor OpenTelemetry Distro sends the telemetry it collects.\n",
    "To copy the connection string:\n",
    "<br>\n",
    "- Go to the Overview pane of your Application Insights resource.\n",
    "- Find your connection string.\n",
    "- Hover over the connection string and select the Copy to clipboard icon.\n",
    "\n",
    "![Migrate from instrumentation keys to connection strings](migrate-from-instrumentation-keys-to-connection-strings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Configure Azure Monitor with AIProject client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to instrument while already instrumented\n",
      "Could not call LangchainInstrumentor().instrument()` since `opentelemetry-instrumentation-langchain` is not installed\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#Use Azure Monitor for OpenTelemetry via project client\n",
    "from azure.identity import DefaultAzureCredential\n",
    "credential = DefaultAzureCredential()\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=credential,\n",
    "    conn_str=project_connection_string,\n",
    ")\n",
    "\n",
    "# Enable Azure Monitor tracing\n",
    "application_insights_connection_string = project_client.telemetry.get_connection_string()\n",
    "if not application_insights_connection_string:\n",
    "    print(\"Application Insights was not enabled for this project.\")\n",
    "    print(\"Enable it via the 'Tracing' tab in your AI Foundry project page.\")\n",
    "    exit()\n",
    "\n",
    "# Enable additional instrumentations if openai and langchain \n",
    "# langchain - https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/trace-local-sdk?tabs=python#enable-tracing-for-langchain\n",
    "# which are not included by Azure Monitor out of the box\n",
    "project_client.telemetry.enable()\n",
    "# you can also use the application insights connection string directly\n",
    "configure_azure_monitor(connection_string=application_insights_connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Set up OpenAIInstrumentor to test Azure Open AI tracing \n",
    "- https://pypi.org/project/opentelemetry-instrumentation-openai-v2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry.instrumentation.openai_v2 import OpenAIInstrumentor\n",
    "\n",
    "# OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT - Optional. Set to `true` to trace the content of chat messages, which may contain personal data. False by default.\n",
    "os.environ[\"OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT\"] = \"true\"\n",
    "\n",
    "# Instrument Azure Open AI API \n",
    "OpenAIInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    aoai_client = AzureOpenAI(\n",
    "        azure_endpoint = aoai_api_endpoint,\n",
    "        api_key        = aoai_api_key,\n",
    "        api_version    = aoai_api_version\n",
    "    )\n",
    "except (ValueError, TypeError) as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI was founded in December 2015 with the mission to ensure that artificial general intelligence (AGI) benefits all of humanity. The founding team included prominent figures in technology, such as Elon Musk and Sam Altman, who aimed to conduct research in AI that prioritized safety, transparency, and collaboration. OpenAI was created as a non-profit organization, with the vision of fostering an open approach to AI research.\n",
      "\n",
      "### Key Milestones and Models:\n",
      "\n",
      "1. **OpenAI Gym (2016)**: One of the early initiatives by OpenAI was the development of OpenAI Gym, a toolkit for developing and comparing reinforcement learning algorithms. It provided a standardized environment for testing AI agents, promoting best practices and sharing results within the community.\n",
      "\n",
      "2. **OpenAI Baselines (2017)**: This set of high-quality implementations of reinforcement learning algorithms further contributed to the research community's advancement in this area.\n",
      "\n",
      "3. **GPT (Generative Pre-trained Transformer) Series**:\n",
      "   - **GPT (2018)**: This was the first version of the Generative Pre-trained Transformer model, which demonstrated that a single model could be pre-trained on a large corpus of text and then fine-tuned for various tasks.\n",
      "   - **GPT-2 (2019)**: This model gained significant attention due to its ability to generate coherent and contextually relevant text. Initially, OpenAI chose not to release the full model due to concerns regarding potential misuse, but they later made it available to the public.\n",
      "   - **GPT-3 (2020)**: With 175 billion parameters, GPT-3 became one of the largest and most powerful language models available at the time. It showcased remarkable abilities in generating human-like text, answering questions, creating content, and engaging in conversation, which led to widespread experimentation and applications across various domains.\n",
      "\n",
      "4. **OpenAI Codex (2021)**: Built on GPT-3's architecture, Codex was fine-tuned specifically for understanding and generating code. It powers GitHub Copilot, a popular coding assistant that helps developers by suggesting code completions, functions, and other programming-related tasks.\n",
      "\n",
      "5. **DALL-E (2021)**: This model revolutionized image generation by creating images from textual descriptions. It demonstrated the potential of combining language understanding with visual creativity, opening new avenues for artistic and commercial applications.\n",
      "\n",
      "6. **CLIP (2021)**: CLIP (Contrastive Language‚ÄìImage Pre-training) could understand images in the context of natural language, allowing for tasks like zero-shot classification. It enabled computers to perform visually based tasks by associating text with images.\n",
      "\n",
      "7. **ChatGPT (2022)**: Deriving from the advances of the GPT series, ChatGPT was tailored for conversational tasks, making it useful for a variety of chat-based applications. The initial versions were based on GPT-3.5, providing more refined interactions.\n",
      "\n",
      "8. **GPT-4 (2023)**: Building upon its predecessors, GPT-4 saw improvements in comprehension, contextual awareness, and task completion. It was further fine-tuned and optimized to provide a more reliable and versatile AI assistant for users.\n",
      "\n",
      "9. **OpenAI API**: Throughout this period, OpenAI developed a robust API, allowing businesses and developers to integrate AI capabilities into their applications. This has led to widespread adoption in various sectors, from customer service chatbots to content generation tools.\n",
      "\n",
      "### Transforming Business and Society\n",
      "OpenAI's models have influenced various sectors, including healthcare, education, entertainment, and more. The organization has emphasized the importance of ethical considerations in AI deployment, conducting research on AI safety, bias mitigation, and aligning AI outcomes with human values.\n",
      "\n",
      "In early 2021, OpenAI transitioned to a \"capped-profit\" model to attract investments while still prioritizing its mission. This approach allows OpenAI to scale operations and develop its technologies while maintaining a commitment to safety and ethical AI practices.\n",
      "\n",
      "Overall, OpenAI's journey reflects its dedication to advancing AI in a responsible manner, balancing innovation with the potential risks associated with powerful AI technologies.\n"
     ]
    }
   ],
   "source": [
    "question = \"Tell me the history of OpenAI and its models.\"\n",
    "\n",
    "user_message = f\"\"\"\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Simple API Call\n",
    "response = aoai_client.chat.completions.create(\n",
    "    model=aoai_deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ],\n",
    "  \n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 inches in a foot.\n"
     ]
    }
   ],
   "source": [
    "response = aoai_client.chat.completions.create(\n",
    "    model=aoai_deployment_name,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How many inches are in a feet?\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace function calling with Azure OpenAI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry import trace\n",
    "#The tracer.start_as_current_span decorator will trace the function call and enable adding additional attributes\n",
    "#to the span in the function implementation. Note that this will trace the function parameters and their values.\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "@tracer.start_as_current_span(\"get_temperature\")  # type: ignore\n",
    "def get_temperature(city: str) -> str:\n",
    "\n",
    "    # Adding attributes to the current span\n",
    "    span = trace.get_current_span()\n",
    "    span.set_attribute(\"requested_city\", city)\n",
    "\n",
    "    if city == \"Seattle\":\n",
    "        return \"75\"\n",
    "    elif city == \"New York City\":\n",
    "        return \"80\"\n",
    "    else:\n",
    "        return \"Unavailable\"\n",
    "\n",
    "\n",
    "# [END trace_function]\n",
    "\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    if city == \"Seattle\":\n",
    "        return \"Nice weather\"\n",
    "    elif city == \"New York City\":\n",
    "        return \"Good weather\"\n",
    "    else:\n",
    "        return \"Unavailable\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "weather_description = ChatCompletionsToolDefinition(\n",
    "    function=FunctionDefinition(\n",
    "        name=\"get_weather\",\n",
    "        description=\"Returns description of the weather in the specified city\",\n",
    "        parameters={\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The name of the city for which weather info is requested\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"city\"],\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "temperature_in_city = ChatCompletionsToolDefinition(\n",
    "    function=FunctionDefinition(\n",
    "        name=\"get_temperature\",\n",
    "        description=\"Returns the current temperature for the specified city\",\n",
    "        parameters={\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The name of the city for which temperature info is requested\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"city\"],\n",
    "        },\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling function `get_weather` with arguments {'city': 'Korea'}\n",
      "Function response = Unavailable\n",
      "Calling function `get_temperature` with arguments {'city': 'Korea'}\n",
      "Function response = Unavailable\n",
      "Model response = I couldn't retrieve the weather and temperature information for Korea. It's possible that the specific location query is too broad. If you have a specific city in Korea, please provide that, and I'll try to get the information for you.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "        SystemMessage(\"You are a helpful assistant.\"),\n",
    "        UserMessage(\"What is the weather and temperature in Korea?\"),\n",
    "    ]\n",
    "\n",
    "tools = [weather_description, temperature_in_city]\n",
    "\n",
    "response = aoai_client.chat.completions.create(model=aoai_deployment_name, messages=messages, tools=tools)\n",
    "\n",
    "if response.choices[0].finish_reason == \"tool_calls\":\n",
    "    messages.append(AssistantMessage(\n",
    "        tool_calls=response.choices[0].message.tool_calls\n",
    "    ))\n",
    "\n",
    "    if response.choices[0].message.tool_calls is not None and len(response.choices[0].message.tool_calls) > 0:\n",
    "        for tool_call in response.choices[0].message.tool_calls:\n",
    "            function_args = json.loads(tool_call.function.arguments.replace(\"'\", '\"'))\n",
    "            print(f\"Calling function `{tool_call.function.name}` with arguments {function_args}\")\n",
    "            callable_func = globals()[tool_call.function.name]\n",
    "            function_response = callable_func(**function_args)\n",
    "            print(f\"Function response = {function_response}\")\n",
    "\n",
    "            messages.append(ToolMessage(\n",
    "                content=function_response,\n",
    "                tool_call_id=tool_call.id\n",
    "            ))\n",
    "\n",
    "        response = aoai_client.chat.completions.create(\n",
    "            model=aoai_deployment_name, \n",
    "            messages=messages,\n",
    "            tools=tools\n",
    "        )\n",
    "\n",
    "print(f\"Model response = {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Set up AIInferenceInstrumentor to test AI Inference tracing\n",
    "- https://pypi.org/project/azure-ai-inference/\n",
    "- https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/trace-local-sdk?tabs=python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.tracing import AIInferenceInstrumentor \n",
    "\n",
    "os.environ[\"AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED\"] = 'true'\n",
    "\n",
    "# Instrument AI Inference API \n",
    "AIInferenceInstrumentor().instrument() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace function calling with Azure OpenAI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_inference_client = ChatCompletionsClient(endpoint=azure_model_inference_endpoint, credential=AzureKeyCredential(aoai_api_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Corporation is an American multinational technology company founded by Bill Gates and Paul Allen on April 4, 1975. Here‚Äôs a brief overview of its history:\n",
      "\n",
      "### 1970s: The Early Years\n",
      "- **Founding**: Microsoft was established in Albuquerque, New Mexico, as a company that developed and sold a version of the BASIC programming language for the Altair 8800, an early personal computer.\n",
      "- **Growth**: The company quickly expanded, and in 1976, it moved to Seattle. By the late 1970s, Microsoft had developed a reputation as a significant player in the software industry.\n",
      "\n",
      "### 1980s: Forming Partnerships and Creating MS-DOS\n",
      "- **IBM Partnership**: In 1980, Microsoft secured a contract with IBM to provide an operating system for its personal computer. Microsoft acquired a system called QDOS (Quick and Dirty Operating System) from a small company, rebranded it as MS-DOS, and delivered it to IBM in 1981.\n",
      "- **Going Public**: Microsoft went public on March 13, 1986, and this event made many of its employees and early investors very wealthy.\n",
      "\n",
      "### 1990s: Dominance with Windows\n",
      "- **Windows Launch**: In 1985, Microsoft launched Windows 1.0, a graphical user interface for MS-DOS. The release of Windows 3.0 in 1990 significantly boosted the company‚Äôs position in the market.\n",
      "- **Microsoft Office**: The company began to integrate its productivity applications into the Microsoft Office suite in 1989, which became a standard in business environments.\n",
      "- **Regulatory Scrutiny**: As Microsoft's market share grew, it faced antitrust scrutiny. In 1998, the U.S. government filed an antitrust lawsuit against Microsoft, alleging that it had engaged in anti-competitive practices, particularly against competitors like Netscape.\n",
      "\n",
      "### 2000s: Technological Evolutions and Challenges\n",
      "- **Windows XP**: Released in 2001, Windows XP was among the most successful operating systems, celebrated for its stability and user-friendly interface.\n",
      "- **Shift to the Internet**: During the early 2000s, Microsoft faced increasing competition from web-based companies and started to adapt its strategy, which included the introduction of products like Windows Server and Microsoft Exchange.\n",
      "- **Antitrust Case Resolution**: The antitrust case concluded in 2001 with a settlement that required structural changes in how Microsoft conducted its business.\n",
      "\n",
      "### 2010s: Mobile and Cloud Initiatives\n",
      "- **Windows 8**: Launched in 2012, Windows 8 was designed to work across multiple devices, including tablets. However, it received mixed reviews.\n",
      "- **Cloud Computing**: Microsoft began shifting its focus toward cloud computing, launching Azure in 2010, which grew to be a key player in the cloud services market.\n",
      "- **Acquisitions**: The company acquired several firms, including Skype in 2011 and the professional networking site LinkedIn in 2016.\n",
      "\n",
      "### 2020s: A New Era\n",
      "- **Windows 10**: Released in 2015, Windows 10 introduced a range of features and was well-received by users. Microsoft also announced that it would be the last version of Windows, positioning future updates as ongoing improvements rather than entirely new versions.\n",
      "- **Emphasis on AI**: In 2020 and 2021, Microsoft began to heavily invest in artificial intelligence, leveraging this technology across its product suite, including Office, Teams, and the Azure platform.\n",
      "- **Windows 11**: Announced in June 2021 and officially released in October 2021, Windows 11 featured a redesigned interface and enhanced performance.\n",
      "\n",
      "### Current Position\n",
      "Microsoft maintains a diverse product lineup, which includes operating systems, software applications, cloud services, gaming products (such as Xbox), and hardware. The company is also recognized for championing security, inclusivity, and sustainability initiatives.\n",
      "\n",
      "Microsoft's ability to adapt to changing tech landscapes, innovate, and expand into new markets has solidified its status as one of the most influential and valuable tech companies in the world.\n"
     ]
    }
   ],
   "source": [
    "question = \"Tell me the history of Microsoft\"\n",
    "\n",
    "user_message = f\"\"\"\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Simple API Call\n",
    "response = ai_inference_client.complete(\n",
    "    model=aoai_deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ],\n",
    "  \n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5,280 feet in a mile.\n"
     ]
    }
   ],
   "source": [
    "response = aoai_client.chat.completions.create(\n",
    "    model=aoai_deployment_name,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How many feet are in a mile?\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling function `get_weather` with arguments {'city': 'Seattle'}\n",
      "Function response = Nice weather\n",
      "Calling function `get_temperature` with arguments {'city': 'Seattle'}\n",
      "Function response = 75\n",
      "Model response = The weather in Seattle is described as nice, with a current temperature of 75¬∞F.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(\"You are a helpful assistant.\"),\n",
    "    UserMessage(\"What is the weather and temperature in Seattle?\"),\n",
    "]\n",
    "\n",
    "response = ai_inference_client.complete(messages=messages, tools=[weather_description, temperature_in_city], model=aoai_deployment_name)\n",
    "\n",
    "if response.choices[0].finish_reason == CompletionsFinishReason.TOOL_CALLS:\n",
    "    # Append the previous model response to the chat history\n",
    "    messages.append(AssistantMessage(tool_calls=response.choices[0].message.tool_calls))\n",
    "    # The tool should be of type function call.\n",
    "    if response.choices[0].message.tool_calls is not None and len(response.choices[0].message.tool_calls) > 0:\n",
    "        for tool_call in response.choices[0].message.tool_calls:\n",
    "            if type(tool_call) is ChatCompletionsToolCall:\n",
    "                function_args = json.loads(tool_call.function.arguments.replace(\"'\", '\"'))\n",
    "                print(f\"Calling function `{tool_call.function.name}` with arguments {function_args}\")\n",
    "                callable_func = globals()[tool_call.function.name]\n",
    "                function_response = callable_func(**function_args)\n",
    "                print(f\"Function response = {function_response}\")\n",
    "                # Provide the tool response to the model, by appending it to the chat history\n",
    "                messages.append(ToolMessage(function_response, tool_call_id=tool_call.id))\n",
    "                # With the additional tools information on hand, get another response from the model\n",
    "        response = ai_inference_client.complete(messages=messages, tools=[weather_description, temperature_in_city], model=aoai_deployment_name)\n",
    "\n",
    "print(f\"Model response = {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Use tracing to view performance and optimize your application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tracing in AI Foundry](tracing_in_ai_foundry.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tracing in AI Foundry](tracing_in_ai_foundry2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. uninstrument the OpenAIInstrumentor and AIInferenceInstrumentor\n",
    "> üìç ***important*** <br>\n",
    "> The OpenAIInstrumentor and AIInferenceInstrumentor are not uninstrumented automatically when the program ends. You must call the uninstrument method to stop tracing.\n",
    "> This is important to avoid memory leaks and ensure that the tracing data is sent to the Azure Monitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry.instrumentation.openai_v2 import OpenAIInstrumentor\n",
    "\n",
    "# Instrument Azure Open AI API \n",
    "OpenAIInstrumentor().uninstrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.tracing import AIInferenceInstrumentor \n",
    "\n",
    "AIInferenceInstrumentor().uninstrument()\n",
    "# [END uninstrument_inferencing]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_trace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
