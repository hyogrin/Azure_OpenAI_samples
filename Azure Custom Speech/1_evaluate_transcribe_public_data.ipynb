{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Eval] Evaluate gpt-4o-transcribe model\n",
    "This sample demonstrates how to evaluate gpt-4o-transcribe model. \n",
    "\n",
    "> âœ¨ ***Note*** <br>\n",
    "> You can test the accuracy of your custom model by creating a test. A test requires a collection of audio files and their corresponding transcriptions. You can compare a custom model's accuracy with a speech to text base model or another custom model. After you get the test results, evaluate the word error rate (WER) compared to speech recognition results. \n",
    "\n",
    "## Prerequisites\n",
    "Configure a Python virtual environment for 3.10 or later: \n",
    " 1. open the Command Palette (Ctrl+Shift+P).\n",
    " 1. Search for Python: Create Environment.\n",
    " 1. select Venv / Conda and choose where to create the new environment.\n",
    " 1. Select the Python interpreter version. Create with version 3.10 or later.\n",
    " 1. ***Prepare huggingface user access token and login with your hugging face account.*** Please check the [reference document](https://huggingface.co/docs/hub/datasets-usage)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'code': 'DeploymentNotFound', 'message': 'The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m deployment_id = \u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini-transcribe\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;66;03m#This will correspond to the custom name you chose for your deployment when you deployed a model.\"\u001b[39;00m\n\u001b[32m     11\u001b[39m audio_test_file = \u001b[33m\"\u001b[39m\u001b[33m./gpt-4o-mini-tts-test-korean1.mp4\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m result = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranscriptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maudio_test_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeployment_id\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/envs/venv_audio/lib/python3.11/site-packages/openai/_utils/_utils.py:279\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/envs/venv_audio/lib/python3.11/site-packages/openai/resources/audio/transcriptions.py:322\u001b[39m, in \u001b[36mTranscriptions.create\u001b[39m\u001b[34m(self, file, model, include, language, prompt, response_format, stream, temperature, timestamp_granularities, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;66;03m# It should be noted that the actual Content-Type header that will be\u001b[39;00m\n\u001b[32m    319\u001b[39m \u001b[38;5;66;03m# sent to the server will contain a `boundary` parameter, e.g.\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[38;5;66;03m# multipart/form-data; boundary=---abc--\u001b[39;00m\n\u001b[32m    321\u001b[39m extra_headers = {\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmultipart/form-data\u001b[39m\u001b[33m\"\u001b[39m, **(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[32m    323\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/audio/transcriptions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtranscription_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTranscriptionCreateParamsStreaming\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtranscription_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTranscriptionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_get_response_format_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTranscriptionStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/envs/venv_audio/lib/python3.11/site-packages/openai/_base_client.py:1276\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1263\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1264\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1271\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1272\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1273\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1274\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1275\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1276\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/envs/venv_audio/lib/python3.11/site-packages/openai/_base_client.py:949\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    946\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    947\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m949\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/envs/venv_audio/lib/python3.11/site-packages/openai/_base_client.py:1057\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1054\u001b[39m         err.response.read()\n\u001b[32m   1056\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1057\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1059\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1060\u001b[39m     cast_to=cast_to,\n\u001b[32m   1061\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1065\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1066\u001b[39m )\n",
      "\u001b[31mNotFoundError\u001b[39m: Error code: 404 - {'error': {'code': 'DeploymentNotFound', 'message': 'The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.'}}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "    \n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2025-03-01-preview\",\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "deployment_id = \"gpt-4o-mini-transcribe\" #This will correspond to the custom name you chose for your deployment when you deployed a model.\"\n",
    "audio_test_file = \"./gpt-4o-mini-tts-test-korean1.mp4\"\n",
    "\n",
    "result = client.audio.transcriptions.create(\n",
    "    file=open(audio_test_file, \"rb\"),            \n",
    "    model=deployment_id\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference.models import SystemMessage\n",
    "from azure.ai.inference.models import UserMessage\n",
    "\n",
    "NUM_SAMPLES = 2\n",
    "\n",
    "topic = f\"\"\"\n",
    "Contoso Electronics call center QnA related expected spoken utterances for {CUSTOM_SPEECH_LANG} and English languages.\n",
    "\"\"\"\n",
    "question = f\"\"\"\n",
    "create {NUM_SAMPLES} lines of jsonl of the topic in {CUSTOM_SPEECH_LANG} and english. jsonl format is required. use 'no' as number and '{CUSTOM_SPEECH_LOCALE}', 'en-US' keys for the languages.\n",
    "only include the lines as the result. Do not include ```jsonl, ``` and blank line in the result. \n",
    "\"\"\"\n",
    "\n",
    "system_message = \"\"\"\n",
    "Generate plain text sentences of #topic# related text to improve the recognition of domain-specific words and phrases.\n",
    "Domain-specific words can be uncommon or made-up words, but their pronunciation must be straightforward to be recognized. \n",
    "Use text data that's close to the expected spoken utterances. The nummber of utterances per line should be 1. \n",
    "Here is examples of the expected format:\n",
    "{\"no\": 1, \"string\": \"string\", \"string\": \"string\"}\n",
    "{\"no\": 2, \"string\": \"string\", \"string\": \"string\"}\n",
    "\"\"\"\n",
    "\n",
    "user_message = f\"\"\"\n",
    "#topic#: {topic}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Simple API Call\n",
    "response = client.chat.completions.create(\n",
    "    model=aoai_deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ],\n",
    "    temperature=0.8,\n",
    "    max_tokens=1024,\n",
    "    top_p=0.1\n",
    ")\n",
    "\n",
    "content = response.choices[0].message.content\n",
    "print(content)\n",
    "print(\"Usage Information:\")\n",
    "#print(f\"Cached Tokens: {response.usage.prompt_tokens_details.cached_tokens}\") #only o1 models support this\n",
    "print(f\"Completion Tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Prompt Tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Total Tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Test based speech models\n",
    "- In order to learn how to quantitatively measure and improve the accuracy of the base speech to text model or your own custom models check this link\n",
    "- https://learn.microsoft.com/en-us/azure/ai-services/speech-service/how-to-custom-speech-evaluate-data?pivots=speech-cli#create-a-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the word error rate (WER) of a base model in Azure AIâ€™s Speech service, follow these steps:\n",
    "\n",
    "Sign in to the Speech Studio:\n",
    "Go to the Azure Speech Studio.\n",
    "Create a Test:\n",
    "Navigate to Custom speech and select your project.\n",
    "Go to Test models and click on Create new test.\n",
    "Select Evaluate accuracy and click Next.\n",
    "Choose an audio + human-labeled transcription dataset. If you donâ€™t have any datasets, upload them in the Speech datasets menu.\n",
    "Select up to two models to evaluate, then click Next.\n",
    "Enter the test name and description, then click Next.\n",
    "Review the test details and click Save and close.\n",
    "Get Test Results:\n",
    "After the test is complete, indicated by the status set to Succeeded, you will see the results, including the WER for each tested model.\n",
    "Evaluate WER:\n",
    "WER is calculated as the sum of insertion, deletion, and substitution errors divided by the total number of words in the reference transcript, multiplied by 100 to get a percentage1.\n",
    "For more detailed instructions, you can refer to this link - https://learn.microsoft.com/en-us/azure/ai-services/speech-service/how-to-custom-speech-evaluate-data?pivots=rest-api.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for the Speech Services REST API\n",
    "base_url = f'https://{SPEECH_REGION}.api.cognitive.microsoft.com/speechtotext'\n",
    "\n",
    "# Headers for authentication\n",
    "headers = {\n",
    "    'Ocp-Apim-Subscription-Key': SPEECH_KEY,\n",
    "    'Content-Type': 'application/json'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the custom speech model ids to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#option1. check the model id from the train a new model (UI) in the Azure Speech Studio. \n",
    "base_model_id = \"8066b5fb-0114-4837-90b6-0c245928a896\"  # Vietnamese base model id\n",
    "\n",
    "#option2. check the model id from the API call\n",
    "base_model = get_latest_base_model(base_url, headers, f\"locale eq '{CUSTOM_SPEECH_LOCALE}' and status eq 'Succeeded'\")\n",
    "\n",
    "# Filter the base models to find the ones that support 'Language' adaptations and have the latest lastActionDateTime\n",
    "filtered_models = [model for model in base_model['values'] if 'properties' in model and 'Language' in model['properties']['features'].get('supportsAdaptationsWith', [])]\n",
    "if filtered_models:\n",
    "\tlatest_model = max(filtered_models, key=lambda x: x['createdDateTime'])\n",
    "\tprint(\"Latest model supporting 'Language' adaptations:\")\n",
    "\tprint(latest_model)\n",
    "else:\n",
    "\tprint(\"No models found that support 'Language' adaptations.\")\n",
    "\n",
    "# Get the latest model ID from the self link for example 8066b5fb-0114-4837-90b6-0c245928a896 is the model id in 'https://swedencentral.api.cognitive.microsoft.com/speechtotext/v3.2/models/base/8066b5fb-0114-4837-90b6-0c245928a896' \n",
    "base_model_id = latest_model['self'].split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the model id from the train a new model (UI) in the Azure Speech Studio. \n",
    "# The base model ids are vary from each language \n",
    "print(\"Latest base model id:\", base_model_id)\n",
    "print(\"custom_model_with_plain_id: \", custom_model_with_plain_id)\n",
    "print(\"custom_model_with_acoustic_id: \", custom_model_with_acoustic_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "import tqdm as notebook_tqdm\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from datasets import load_dataset\n",
    "\n",
    "def create_commonvoice_zip(language: str,\n",
    "                           subset: str = \"test\",\n",
    "                           output_sample_rate: int = 16000,\n",
    "                           num_samples: int = None):\n",
    "    \"\"\"\n",
    "    Create a ZIP file containing:\n",
    "      - WAV files (PCM 16-bit, mono, `output_sample_rate` Hz)\n",
    "      - A manifest.txt file listing filename + transcript.\n",
    "\n",
    "    :param language:            Language code, e.g. \"hi\", \"en\", \"vi\", ...\n",
    "    :param subset:              Subset of the Common Voice dataset, e.g. \"test\", \"train\", etc.\n",
    "    :param output_sample_rate:  Target sample rate (Hz) for the output WAV files.\n",
    "    :param num_samples:         Number of samples to process (useful for sampling). \n",
    "                                If None, process the entire subset.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Load the specified subset of the dataset\n",
    "    dataset = load_dataset(\"mozilla-foundation/common_voice_15_0\", language, split=subset)\n",
    "    \n",
    "    # 2. If sampling is requested, shuffle and select\n",
    "    if num_samples is not None and num_samples < len(dataset):\n",
    "        dataset = dataset.shuffle(seed=42)           # Shuffle for random selection\n",
    "        dataset = dataset.select(range(num_samples)) # Take first `num_samples` items\n",
    "    \n",
    "    total_items = len(dataset)\n",
    "    print(f\"Processing {total_items} items from '{language}' subset='{subset}'.\")\n",
    "\n",
    "    # 3. Create a folder to store intermediate WAV files + manifest\n",
    "    folder_name = f\"{language}_commonvoice_wavs\"\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    manifest_path = os.path.join(folder_name, \"manifest.txt\")\n",
    "\n",
    "    # 4. Convert each MP3 to WAV (resampled) and write the manifest\n",
    "    timestamp_str = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    with open(manifest_path, \"w\", encoding=\"utf-8\") as manifest_file:\n",
    "        for idx, sample in enumerate(dataset):\n",
    "            \n",
    "            # get the filename\n",
    "            wav_filename = f\"{idx+1}_{language}_{timestamp_str}.wav\"\n",
    "            wav_path = os.path.join(folder_name, wav_filename)\n",
    "\n",
    "            audio_array = sample[\"audio\"][\"array\"]\n",
    "            original_sr = sample[\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "            # Convert to float32 if not already\n",
    "            audio_array = audio_array.astype(np.float32)\n",
    "            # Use librosa to resample from original_sr to output_sample_rate\n",
    "            if original_sr != output_sample_rate:\n",
    "                audio_array = librosa.resample(\n",
    "                    audio_array, \n",
    "                    orig_sr=original_sr,\n",
    "                    target_sr=output_sample_rate\n",
    "                )\n",
    "            # --------------------------------\n",
    "\n",
    "            # 5. Write the audio to a mono, 16-bit WAV at the desired sample rate\n",
    "            sf.write(\n",
    "                wav_path,\n",
    "                audio_array,\n",
    "                samplerate=output_sample_rate,\n",
    "                subtype='PCM_16'\n",
    "            )\n",
    "\n",
    "            # 6. Append the line to the manifest (filename <tab> sentence)\n",
    "            text = sample.get(\"sentence\", \"\").replace(\"\\t\", \" \")\n",
    "            manifest_file.write(f\"{wav_filename}\\t{text}\\n\")\n",
    "\n",
    "    # 7. Create the ZIP archive\n",
    "    timestamp_str = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    zip_filename = f\"{language}_commonvoice_{subset}_{timestamp_str}.zip\"\n",
    "    \n",
    "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Add manifest.txt\n",
    "        zipf.write(manifest_path, arcname=\"manifest.txt\")\n",
    "\n",
    "        # Add all WAV files\n",
    "        for root, dirs, files in os.walk(folder_name):\n",
    "            for file in files:\n",
    "                if file.endswith(\".wav\"):\n",
    "                    full_path = os.path.join(root, file)\n",
    "                    zipf.write(full_path, arcname=file)\n",
    "\n",
    "    print(f\"Created {zip_filename}, containing {total_items} samples from '{language}' subset='{subset}'.\")\n",
    "    return zip_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "data_folder = \"eval_dataset\"\n",
    "\n",
    "#create_commonvoice_zip(\"en\", subset=\"test\", output_sample_rate=16000, num_samples=100)\n",
    "#create_commonvoice_zip(\"ko\", subset=\"test\", output_sample_rate=16000, num_samples=100)\n",
    "zip_filename = create_commonvoice_zip(\"ko\", subset=\"test\", num_samples=100)\n",
    "\n",
    "shutil.move(zip_filename, os.path.join(data_folder, zip_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload zip files to a storage account and generate content urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "account_name = os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\")\n",
    "account_key = os.getenv(\"AZURE_STORAGE_ACCOUNT_KEY\")\n",
    "container_name = os.getenv(\"AZURE_STORAGE_CONTAINER_NAME\")\n",
    "\n",
    "uploaded_files, url = upload_dataset_to_storage(data_folder, container_name, account_name, account_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind=\"Acoustic\"\n",
    "description = f\"[eval] Dataset for evaluation the {CUSTOM_SPEECH_LANG} base model\"\n",
    "dataset_ids = {}\n",
    "\n",
    "for display_name in uploaded_files:\n",
    "    dataset_ids[display_name] = create_dataset(base_url, headers, project_id, url[display_name], kind, display_name, description, CUSTOM_SPEECH_LOCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_upload_monitor_status(base_url, headers, get_method, id):\n",
    "    with tqdm(total=4, desc=\"Running Status\", unit=\"step\") as pbar:\n",
    "        status = get_method(base_url, headers, id)\n",
    "        if status == \"NotStarted\":\n",
    "            pbar.update(1)\n",
    "        while status != \"Succeeded\" and status != \"Failed\":\n",
    "            if status == \"Running\" and pbar.n < 2:\n",
    "                pbar.update(1)\n",
    "            print(f\"Current Status: {status}\")\n",
    "            time.sleep(10)\n",
    "            status = get_method(base_url, headers, id)\n",
    "        while(pbar.n < 3):\n",
    "            pbar.update(1)\n",
    "        print(f\"Current Status: {status}, Operation Completed\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for display_name in uploaded_files:\n",
    "    dataset_upload_monitor_status(base_url, headers, get_dataset_status, dataset_ids[display_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test accuracy of the trained Custom Speech model creating evaluations (tests)\n",
    "\n",
    "> âœ¨ ***Note*** <br>\n",
    "> If you find an error in the evaluation, check the status of data files uploaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_create_evaluation(base_url, headers, project_id, dataset_id, model1_id, model2_id, display_name, description, locale):\n",
    "    \"\"\"\n",
    "    Creates an evaluation job using the dataset and Vietnamese base model.\n",
    "    \"\"\"\n",
    "    evaluations_url = f'{base_url}/evaluations?api-version=2024-11-15'\n",
    "    print(evaluations_url)\n",
    "    print(project_id, dataset_id, model1_id, model2_id)\n",
    "\n",
    "    evaluation_body = {\n",
    "        \"model1\": {\n",
    "            \"self\": f'{base_url}/models/{model1_id}'\n",
    "        },\n",
    "        \"model2\": {\n",
    "            \"self\": f'{base_url}/models/{model2_id}'\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"self\": f'{base_url}/datasets/{dataset_id}'\n",
    "        },\n",
    "        \"project\": {\n",
    "            \"self\": f'{base_url}/projects/{project_id}'\n",
    "        },\n",
    "        \"displayName\": display_name,\n",
    "        \"description\": description,\n",
    "        \"locale\": locale,\n",
    "        \n",
    "    }\n",
    "    \n",
    "    print(evaluation_body)\n",
    "\n",
    "    response = requests.post(evaluations_url, headers=headers, json=evaluation_body)\n",
    "    response.raise_for_status()\n",
    "    evaluation = response.json()\n",
    "    evaluation_id = evaluation['self'].split('/')[-1]\n",
    "    print(f'Evaluation job created with ID: {evaluation_id}')\n",
    "    return evaluation_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = f\"{CUSTOM_SPEECH_LOCALE} Evaluation of the {CUSTOM_SPEECH_LANG} base and custom model\"\n",
    "evaluation_ids = {}\n",
    "for display_name in uploaded_files:\n",
    "    evaluation_ids[display_name] = new_create_evaluation(\n",
    "        base_url=base_url,\n",
    "        headers=headers,\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_ids[display_name],\n",
    "        model1_id=base_model_id,\n",
    "        model2_id=custom_model_with_acoustic_id,\n",
    "        display_name=f'vi_eval_base_vs_custom_{display_name}',\n",
    "        description=description,\n",
    "        locale=CUSTOM_SPEECH_LOCALE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for display_name in uploaded_files:\n",
    "    monitor_status(base_url, headers, get_evaluation_status, evaluation_ids[display_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Print evaluation result with WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect WER results for each dataset\n",
    "wer_results = []\n",
    "eval_title = \"Evaluation Results for base model and custom model: \"\n",
    "for display_name in uploaded_files:\n",
    "    eval_info = get_evaluation_results(base_url, headers, evaluation_ids[display_name])\n",
    "    print(eval_info)\n",
    "    eval_title = eval_title + display_name + \" \"\n",
    "    wer_results.append({\n",
    "            'Dataset': display_name,\n",
    "            'WER_base_model': eval_info['properties']['wordErrorRate1'],\n",
    "            'WER_custom_model': eval_info['properties']['wordErrorRate2'],\n",
    "            \n",
    "    })\n",
    "# Create a DataFrame to display the results\n",
    "print(eval_info)\n",
    "wer_df = pd.DataFrame(wer_results)\n",
    "print(eval_title)\n",
    "print(wer_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a markdown file for table scoring results\n",
    "md_table_scoring_result(base_url, headers, evaluation_ids, uploaded_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
