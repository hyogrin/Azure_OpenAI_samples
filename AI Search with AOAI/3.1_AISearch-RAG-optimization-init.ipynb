{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Search with RAG optimization\n",
    "This document illustrates an example workflow for how to optimize Azure AI Search for RAG use cases to enhance the quality of document search. \n",
    "\n",
    "# Objective\n",
    "See if your RAG app's quality metrics improve using only the capabilities of Azure AI Search\n",
    "- Enhanced Vector (text-embedding3-large)\n",
    "- Multivector\n",
    "- BM25 + Vector\n",
    "- BM25 + Vector + Semantic Reranking\n",
    "- BM25 + Vector + Semantic Reranking + Query Rewrite\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "Configure a Python virtual environment for 3.10 or later: \n",
    " 1. open the Command Palette (Ctrl+Shift+P).\n",
    " 1. Search for Python: Create Environment.\n",
    " 1. select Venv / Conda and choose where to create the new environment.\n",
    " 1. Select the Python interpreter version. Create with version 3.10 or later.\n",
    "\n",
    "For a dependency installation, run the code below to install the packages required to run it. \n",
    "\n",
    "```bash\n",
    "# Create a virtual environment\n",
    "python -m venv venv\n",
    "\n",
    "# Activate the virtual environment\n",
    "# On Windows\n",
    "venv\\Scripts\\activate\n",
    "\n",
    "# On macOS/Linux\n",
    "source venv/bin/activate\n",
    "\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "## Set up your environment\n",
    "Git clone the repository to your local machine. \n",
    "\n",
    "```bash\n",
    "git clone https://github.com/hyogrin/Azure_OpenAI_samples.git\n",
    "```\n",
    "\n",
    "Create an .env file based on the .env-sample file. Copy the new .env file to the folder containing your notebook and update the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "import sys\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    AzureOpenAIVectorizer,\n",
    "    SemanticConfiguration,\n",
    "    SemanticSearch,\n",
    "    SemanticPrioritizedFields,\n",
    "    AzureOpenAIVectorizerParameters,\n",
    "    SemanticField,\n",
    "    SearchIndex,\n",
    "    VectorSearchAlgorithmMetric,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    HnswParameters,\n",
    "    ExhaustiveKnnAlgorithmConfiguration,\n",
    "    ExhaustiveKnnParameters,\n",
    "    ScoringProfile,\n",
    "    TextWeights\n",
    ")\n",
    "\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "\n",
    "load_dotenv(override=True)   \n",
    "\n",
    "search_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "admin_key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "small3_deployment = os.getenv(\"AZURE_OPENAI_3_SMALL_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "large3_deployment = os.getenv(\"AZURE_OPENAI_3_LARGE_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "gpt_chat_deployment = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "\n",
    "print(f\"search_endpoint: {search_endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Read the JSON file\n",
    "# faq_data = pd.read_json('data/Extracted_FAQ_v3.json')\n",
    "\n",
    "# # Rename columns to lowercase and 'Num' to 'id'\n",
    "# faq_data.columns = faq_data.columns.str.lower()\n",
    "# faq_data.rename(columns={'num': 'id'}, inplace=True)\n",
    "\n",
    "# # Add a new column 'title' combining 'sheet_name', 'category', and 'type'\n",
    "# faq_data['title'] = faq_data['sheet_name'] + \" \" + faq_data['category'] + \" \" + faq_data['type']\n",
    "\n",
    "# # Display the modified dataframe\n",
    "# print(faq_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# from openai import AzureOpenAI\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "\n",
    "# aoai_api_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "# aoai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "# aoai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "# aoai_deployment_name = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "# if not aoai_api_version:\n",
    "#     aoai_api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "    \n",
    "# try:\n",
    "#     client = AzureOpenAI(\n",
    "#         azure_endpoint = aoai_api_endpoint,\n",
    "#         api_key        = aoai_api_key,\n",
    "#         api_version    = aoai_api_version\n",
    "#     )\n",
    "# except (ValueError, TypeError) as e:\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure the OpenAI client is already initialized as `client`\n",
    "\n",
    "# # Function to translate text using OpenAI\n",
    "# def translate_to_korean(text):\n",
    "#     try:\n",
    "#         response = client.chat.completions.create(\n",
    "#             model=aoai_deployment_name,\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": \"translate the following text to Korean.\"},\n",
    "#                 {\"role\": \"user\", \"content\": text},\n",
    "#             ],\n",
    "#             temperature=0.3,\n",
    "#             max_tokens=1000,\n",
    "#         )\n",
    "#         return response.choices[0].message.content.strip()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error translating text: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Add a new column 'question_ko' with Korean translations\n",
    "# faq_data['title'] = faq_data['title'].apply(translate_to_korean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faq_data[['id', 'category', 'type', 'title', 'content']].to_json(\n",
    "#     'data/rag_sample_data_ko.jsonl', \n",
    "#     orient='records', \n",
    "#     lines=True, \n",
    "#     force_ascii=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faq_data[['id', 'category', 'type', 'question', 'answer']].to_json(\n",
    "#     'data/rag_sample_qna_ko.jsonl', \n",
    "#     orient='records', \n",
    "#     lines=True, \n",
    "#     force_ascii=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faq_data.to_json('data/rag_sample_data_ko.jsonl', orient='records', lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings\n",
    "- Insert Azure AI Search indexes by reading data, creating OpenAI embeddings, and exporting to a valid format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RETRIES = 3\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),        # Azure OpenAI base URL\n",
    "    api_key        = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version    = os.getenv(\"AZURE_OPENAI_CHAT_API_VERSION\"),\n",
    "    max_retries    = MAX_RETRIES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/rag_sample_data_ko.jsonl -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Document Embeddings using OpenAI Small 3\n",
    "\n",
    "# Read the text-sample.json\n",
    "# with open(\"text-sample.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "#     input_data = json.load(file)\n",
    "\n",
    "df_input_data = pd.read_json(os.path.join(os.getcwd(),'data/rag_sample_data_ko.jsonl'), lines=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.options.mode.chained_assignment = None #https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#evaluation-order-matters\n",
    "\n",
    "# s is input text\n",
    "def normalize_text(s, sep_token = \" \\n \"):\n",
    "    s = re.sub(r'\\s+',  ' ', s).strip()\n",
    "    s = re.sub(r\". ,\",\"\",s)\n",
    "    # remove all instances of multiple spaces\n",
    "    s = s.replace(\"..\",\".\")\n",
    "    s = s.replace(\". .\",\".\")\n",
    "    s = s.replace(\"\\n\", \"\")\n",
    "    s = s.strip()\n",
    "    \n",
    "    return s\n",
    "\n",
    "df_input_data['content']= df_input_data[\"content\"].apply(lambda x : normalize_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To take advantage of the Embedding API provided by Azure OpenAI, we check that the document does not have more than 8,192 tokens of text in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "df_input_data['n_tokens'] = df_input_data[\"content\"].apply(lambda x: len(tokenizer.encode(x)))\n",
    "df_input_data = df_input_data[df_input_data.n_tokens<8192]\n",
    "len(df_input_data)\n",
    "df_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After completing the verification, delete the columns that are no longer needed, and save the data for insertion into Azure AI Search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_data = df_input_data.drop('n_tokens', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Document Embeddings using OpenAI Small 3\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "# Function to generate embeddings for title and content fields, also used for query embeddings\n",
    "def get_embedding(text, model=small3_deployment): # model = \"deployment_name\"\n",
    "    return client.embeddings.create(input = [text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model should be set to the deployment name you chose when you deployed the text-embedding-small3 (Version 2) model\n",
    "df_input_data['content_vector'] = df_input_data[\"content\"].apply(lambda x : get_embedding (x, model = small3_deployment)) \n",
    "df_input_data['content_vector_3'] = df_input_data[\"content\"].apply(lambda x : get_embedding (x, model = large3_deployment)) \n",
    "df_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_data.to_csv(os.path.join(os.getcwd(),'data/embedding_input_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your search index\n",
    "Create your search index schema and vector search configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a search index\n",
    "key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "credential = AzureKeyCredential(key)\n",
    "\n",
    "index_client = SearchIndexClient(endpoint=search_endpoint, credential=credential)\n",
    "fields = [\n",
    "        SimpleField(name=\"id\", key=True, type=SearchFieldDataType.String, sortable=True, filterable=True, facetable=False),  \n",
    "        SearchableField(name=\"category\", type=SearchFieldDataType.String, filterable=True, facetable=True),\n",
    "        SearchableField(name=\"type\", type=SearchFieldDataType.String, filterable=True, facetable=True),\n",
    "        SearchableField(name=\"title\", type=SearchFieldDataType.String, filterable=True, facetable=True),\n",
    "        SearchableField(name=\"content\", type=SearchFieldDataType.String, filterable=True, facetable=True, analyzer_name=\"ko.microsoft\"),\n",
    "        SearchField(\n",
    "            name=\"content_vector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            vector_search_dimensions=1536,\n",
    "            vector_search_profile_name=\"myHnswProfile\",  # Ensure vector_search_profile is set\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"content_vector_3\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            vector_search_dimensions=3072,\n",
    "            vector_search_profile_name=\"myHnswProfile\",  # Ensure vector_search_profile is set\n",
    "        )]\n",
    "# Configure the vector search configuration  \n",
    "vector_search = VectorSearch(  \n",
    "    algorithms=[  \n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"myHnsw\",\n",
    "            kind=VectorSearchAlgorithmKind.HNSW,\n",
    "            parameters=HnswParameters(\n",
    "                m=10,\n",
    "                ef_construction=1000,\n",
    "                ef_search=500,\n",
    "                metric=VectorSearchAlgorithmMetric.COSINE\n",
    "            )\n",
    "        ),\n",
    "        ExhaustiveKnnAlgorithmConfiguration(\n",
    "            name=\"myExhaustiveKnn\",\n",
    "            kind=VectorSearchAlgorithmKind.EXHAUSTIVE_KNN,\n",
    "            parameters=ExhaustiveKnnParameters(\n",
    "                metric=VectorSearchAlgorithmMetric.COSINE\n",
    "            )\n",
    "        )\n",
    "    ],  \n",
    "    profiles=[  \n",
    "        VectorSearchProfile(  \n",
    "            name=\"myHnswProfile\",  \n",
    "            algorithm_configuration_name=\"myHnsw\",  \n",
    "            vectorizer=\"myOpenAI\",  \n",
    "        ),\n",
    "        VectorSearchProfile(\n",
    "            name=\"myExhaustiveKnnProfile\",\n",
    "            algorithm_configuration_name=\"myExhaustiveKnn\",\n",
    "            vectorizer=\"myOpenAI\",  \n",
    "        )\n",
    "        \n",
    "    ],  \n",
    "    vectorizers=[  \n",
    "        AzureOpenAIVectorizer(  \n",
    "            vectorizer_name=\"myOpenAI_vectorizer\",  \n",
    "            kind=\"azureOpenAI\",  \n",
    "            parameters=AzureOpenAIVectorizerParameters(  \n",
    "                resource_url=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),  \n",
    "                deployment_name=small3_deployment,\n",
    "                model_name=small3_deployment,\n",
    "                api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "            ),\n",
    "            \n",
    "        ),  \n",
    "    ],  \n",
    ")  \n",
    "\n",
    "\n",
    "    \n",
    "  \n",
    "semantic_config = SemanticConfiguration(  \n",
    "    name=\"my-semantic-config\",  \n",
    "        prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"title\"),\n",
    "        content_fields=[SemanticField(field_name=\"content\")]  \n",
    "    ),  \n",
    ")\n",
    "\n",
    "# New scoring profile for keyword search\n",
    "# https://learn.microsoft.com/en-us/python/api/azure-search-documents/azure.search.documents.indexes.models.scoringprofile?view=azure-python\n",
    "scoring_profiles = [  \n",
    "    ScoringProfile(  \n",
    "        name=\"my-scoring-profile\",\n",
    "        text_weights = TextWeights(\n",
    "        \tweights = {\n",
    "                'category' : 1,\n",
    "                'type' : 1,\n",
    "                'title' : 3,\n",
    "                'content' : 1\n",
    "            }\n",
    "        )\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "# Create the semantic search with the configuration  \n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])  \n",
    "\n",
    "# Create a suggester that is a configuration in an index that specifies which fields should be used to populate autocomplete and suggested matches.\n",
    "suggesters= [\n",
    "    {\n",
    "        \"name\": \"sg\",\n",
    "        \"searchMode\": \"analyzingInfixMatching\",\n",
    "        \"sourceFields\": [\"title\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    existing_index = index_client.get_index(index_name)\n",
    "    print(f\"index '{index_name}' exists. Proceed the index update.\")\n",
    "\n",
    "    index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search, semantic_search=semantic_search,suggesters=suggesters, scoring_profiles=scoring_profiles)\n",
    "    result = index_client.create_or_update_index(index)\n",
    "    print(f\"index '{result.name}' updated.\")\n",
    "    \n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"index ( {index_name} ) does not exist. Create a new index.\")\n",
    "\n",
    "    index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search, semantic_search=semantic_search,suggesters=suggesters, scoring_profiles=scoring_profiles)\n",
    "    result = index_client.create_or_update_index(index)\n",
    "    print(f\"index ( {result.name} ) creation completed.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert text and embeddings into vector store\n",
    "Add texts and metadata from the JSON data to the vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data_embeddings.csv to search_client.upload_documents to create the index \n",
    "documents = df_input_data.to_dict(orient='records')\n",
    "for doc in documents:\n",
    "    doc['id'] = str(doc['id'])\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint, index_name=index_name, credential=credential\n",
    ")\n",
    "result = search_client.upload_documents(documents)\n",
    "\n",
    "print(f\"Uploaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_aisearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
